{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### (pavellexyr) The Reddit Climate Change Dataset: ###\n",
        "Retrieved from: https://www.kaggle.com/datasets/pavellexyr/the-reddit-climate-change-dataset\n",
        "\n",
        "There are two .csv files, one containing comments from Reddit on climate change and the other containing posts on climate change. For this project, we will only be using the .csv file with comments as there is already sentiment analysis done on the 4.6 million observations collected.\n",
        "\n",
        "An observation in this dataset consists of:\n",
        "* Type of datapoint (comment)\n",
        "* Unique ID of the comment\n",
        "* Unique ID of the comment’s subreddit\n",
        "* The name of the subreddit the comment was found on\n",
        "* If the comment’s subreddit is NSFW\n",
        "* The timestamp (UTC) of the comment\n",
        "* Permalink to the comment\n",
        "* Body text of the comment\n",
        "* Analyzed sentiment for the comment as a continuous value from [-1, 1]\n",
        "* Comment’s score (votes on Reddit)\n",
        "\n",
        "### (cosmos98) Twitter and Reddit Sentimental analysis Dataset: ###\n",
        "Retrieved from: https://www.kaggle.com/datasets/cosmos98/twitter-and-reddit-sentimental-analysis-dataset\n",
        "\n",
        "There are two .csv files, one containing comments from Reddit (36k observations) and the other containing tweets from Twitter (162k observations). The Twitter dataset was extracted with the focus on tweets people made about the Indian Prime Minister Narendra Modi. The Reddit dataset has no indication of having a specific area or topic they were sourced from. \n",
        "\n",
        "Both datasets only have two variables:\n",
        "* The text of the comment or tweet\n",
        "* The category/sentiment of the text {-1, 0, 1}\n",
        "\n",
        "### (tirendazacademy) FIFA World Cup 2022 Tweets:  ###\n",
        "Retrieved from: https://www.kaggle.com/datasets/tirendazacademy/fifa-world-cup-2022-tweets\n",
        "\n",
        "There is one .csv file containing tweets regarding the 2022 FIFA World Cup, a dataset of about 22k observations.\n",
        "\n",
        "An observation in this dataset contains:\n",
        "* ID (index) of the observation\n",
        "* Date the tweet was created\n",
        "* Number of likes the tweet had\n",
        "* Source of the tweet (Twitter of iPhone, Twitter for Android)\n",
        "* The body text of the tweet\n",
        "* Sentiment of the tweet as strings: “positive”, “neutral”, or “negative”\n"
      ],
      "metadata": {
        "id": "OsyUP33PKbtB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For all the datasets discussed above, there are only two variables we are concerned with: the text of the comment/tweet and the sentiment the text was already given. Due to some datasets being incredibly large, only the first 100k observations of each dataset will be used. \n",
        "\n",
        "All datasets will undergo further data cleaning. To the best of our ability, we will filter our dataset to only have English detected text using the langdetect library and regex. Other unusable observations (such as rows containing NaN values) will also be excluded. This results in slightly less than the upper limit of 100k observations initially taken from each dataset. In addition, the existing numeric labels for sentiment some datasets may have will be changed to string values of “positive”, “negative”, and “neutral” to be consistent with each other and only have 3 total classes for classification. \n",
        "\n",
        "The full code for cleaning the files used up to this point are in the “COGS118A replacement data cleaning.ipynb” file in this repository. Below will be code snippets, mainly of the functions created to clean the data. Demonstration will also be available in the other file as it is simply too large to include here. For demonstration purposes, these will just be example variable names instead of what was actually used in practice. \n"
      ],
      "metadata": {
        "id": "P3CQ_PPVLPmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Libraries and global variables to be used for cleaning and limiting collected data to 100k observations at most. \n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "row_count = 1000\n",
        "max_obv = 100\n",
        "\n",
        "#https://pypi.org/project/langdetect/\n",
        "### uncomment this to install, then comment and restart kernel ###\n",
        "# %%capture\n",
        "# !pip install langdetect\n",
        "\n",
        "from langdetect import detect\n",
        "from langdetect.lang_detect_exception import LangDetectException\n",
        "### regex for more lang checking\n",
        "import re"
      ],
      "metadata": {
        "id": "OuOw8mHvLWCP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Taking the first 100k observations of a dataset and putting it into an initial DataFrame object."
      ],
      "metadata": {
        "id": "0IUoJ6TgYPXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_text = []\n",
        "data_sentiment = []\n",
        "i = 0\n",
        "for chunk in pd.read_csv('dataset.csv', chunksize=row_count):\n",
        "  if i < max_obv:\n",
        "    data_text += chunk['text'].tolist()\n",
        "    data_sentiment += chunk['sentiment'].tolist()\n",
        "    i += 1\n",
        "  else:\n",
        "    break;"
      ],
      "metadata": {
        "id": "_Z8T6rtyMD3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(data={'text': data_text, 'sentiment': data_sentiment})\n",
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "GSiWZWxsaPgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to validate a text body. Valid text will be non-empty strings that are not solely whitespace of at least length 1. Regex and langdetect is used to keep observations that have Latin characters (so that it may be able to filter out text using solely Korean characters for example)."
      ],
      "metadata": {
        "id": "pvX6BEtraa9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_line(line):\n",
        "    if not line:\n",
        "        return np.nan\n",
        "    if line == \"\":\n",
        "        return np.nan\n",
        "    if not bool(line.strip()):\n",
        "        return np.nan\n",
        "    if len(line) < 1:\n",
        "        return np.nan\n",
        "    \n",
        "    if bool(re.match('^(?=.*[a-zA-Z])', line)):\n",
        "        try:\n",
        "            if detect(line) != 'en':\n",
        "                return np.nan\n",
        "        except LangDetectException:\n",
        "            return np.nan\n",
        "    return True"
      ],
      "metadata": {
        "id": "UnV0rg-Xa4KS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to check if text body is English (detect returns 'en') for the entire dataset. \n",
        "* text_col takes a DataFrame.Series: df['text']\n",
        "* sentiment_col takes a DataFrame.Series: df['sentiment']\n",
        "Returns 3 lists of the same length, truncating the last chunk of observations that are less than 1000."
      ],
      "metadata": {
        "id": "bX9-zm9ja-3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_en(text_col, sentiment_col):\n",
        "    en_text = text_col.tolist()\n",
        "    en_sentiment = sentiment_col.tolist()\n",
        "    lang = []\n",
        "    \n",
        "    start = 0\n",
        "    for i in np.arange(row_count, len(en_text), row_count):\n",
        "        #observations <1000 at the end will be lost but impact is negligible\n",
        "        #!!!uncomment print statement below to show progress (recommended)!!!\n",
        "#         print(start, i)\n",
        "        lang += [validate_line(x) for x in en_text[start:i]]\n",
        "        start = i\n",
        "    print(\"Finished English check\")\n",
        "    ### all three return values should be of the same length\n",
        "    return en_text[0:len(lang)], en_sentiment[0:len(lang)], lang"
      ],
      "metadata": {
        "id": "eLBYnWoWbwjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Putting the returned lists from check_en into a new DataFrame. df['english'] will be dropped after removing all rows with NaN values (non-English, non valid text bodies)."
      ],
      "metadata": {
        "id": "YjPb4WC_cA3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "en_data_text, en_data_sentiment, en_data_lang = check_en(df['text'], df['sentiment'])\n",
        "\n",
        "en_df = pd.DataFrame(data={'text': en_data_text, 'sentiment': en_data_sentiment, 'english': en_data_lang})\n",
        "en_df = en_df.dropna()\n",
        "en_df = en_df.drop(columns=['english'])"
      ],
      "metadata": {
        "id": "tmLd7e1-cWhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to change numeric sentiment values into string values, otherwise it will just return the input value (if it was already string)."
      ],
      "metadata": {
        "id": "HJlVz4iOc56E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentiment_to_string(sentiment):\n",
        "    if type(sentiment) == int or type(sentiment) == float:\n",
        "        if sentiment < 0:\n",
        "            return \"negative\"\n",
        "        if sentiment > 0:\n",
        "            return \"positive\"\n",
        "        return \"neutral\"\n",
        "    else:\n",
        "        return sentiment"
      ],
      "metadata": {
        "id": "SeY9mhQLdELN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final clean dataset as a DataFrame after this line."
      ],
      "metadata": {
        "id": "bKgjImtrdHfZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "en_df['sentiment'] = en_df['sentiment'].apply(sentiment_to_string)"
      ],
      "metadata": {
        "id": "1byQ-7UzdK6a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}